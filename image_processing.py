# -*- coding: utf-8 -*-
"""image_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12NmkC0sRd0LMmp1cIseTDSnpEf016GQl
"""

# image_processing

import cv2
import numpy as np
import os
import multiprocessing

def find_equal_threshold(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    hist = np.bincount(gray_image.ravel(), minlength=256)
    cdf = np.cumsum(hist)
    threshold = np.where(cdf >= cdf[-1] // 2)[0][0]
    return threshold

def convert_to_equal_black_white(image, threshold):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    bw_image = np.where(gray_image > threshold, 255, 0).astype(np.uint8)
    return bw_image

def apply_averaging_blur(image, kernel_size=(5, 5)):
    blurred_image = cv2.blur(image, kernel_size)
    return blurred_image

def add_salt_pepper_noise_based_on_bw(image, bw_image):
    total_pixels = image.shape[0] * image.shape[1]
    black_indices = np.where(bw_image == 0)
    num_black_pixels = black_indices[0].size
    num_noise_pixels = num_black_pixels // 10
    noise_indices = np.random.choice(total_pixels, num_noise_pixels, replace=False)
    rows, cols = np.unravel_index(noise_indices, (image.shape[0], image.shape[1]))
    salt_indices = rows[:num_noise_pixels // 2], cols[:num_noise_pixels // 2]
    image[salt_indices] = [255, 255, 255]
    pepper_indices = rows[num_noise_pixels // 2:], cols[num_noise_pixels // 2:]
    image[pepper_indices] = [0, 0, 0]
    return image

def create_subfolders(folder_path, subfolders):
    for subfolder in subfolders:
        os.makedirs(os.path.join(folder_path, subfolder), exist_ok=True)

def process_image(image_path, folder_path, subfolders):
    image = cv2.imread(image_path)
    bw_image = convert_to_equal_black_white(image, find_equal_threshold(image))
    cv2.imwrite(os.path.join(folder_path, subfolders[0], os.path.basename(image_path)), bw_image)
    cv2.imwrite(os.path.join(folder_path, subfolders[1], os.path.basename(image_path)), apply_averaging_blur(image))
    cv2.imwrite(os.path.join(folder_path, subfolders[2], os.path.basename(image_path)), add_salt_pepper_noise_based_on_bw(image, bw_image))

# starmap parallel

def process_images_multiproc_starmap_async(folder_path):
    subfolders = ['BlackWhite', 'Blurred', 'Noise']
    create_subfolders(folder_path, subfolders)
    tasks = []
    for image_name in os.listdir(folder_path):
        if image_name.lower().endswith('.jpg'):
            image_path = os.path.join(folder_path, image_name)
            tasks.append((image_path, folder_path, subfolders))
    pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 2))
    result = pool.starmap_async(process_image, tasks)
    result.wait()
    pool.close()
    pool.join()

# apply_parallel

def process_images_multiproc_apply_async(folder_path):
    subfolders = ['BlackWhite', 'Blurred', 'Noise']
    create_subfolders(folder_path, subfolders)
    pool = multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 2))
    for image_name in os.listdir(folder_path):
        if image_name.lower().endswith('.jpg'):
            image_path = os.path.join(folder_path, image_name)
            args = (image_path, folder_path, subfolders)
            pool.apply_async(process_image, args=args)
    pool.close()
    pool.join()

# map_parallel

def process_image_map(args):
    image_path, folder_path, subfolders = args
    image = cv2.imread(image_path)
    bw_image = convert_to_equal_black_white(image, find_equal_threshold(image))
    cv2.imwrite(os.path.join(folder_path, subfolders[0], os.path.basename(image_path)), bw_image)
    cv2.imwrite(os.path.join(folder_path, subfolders[1], os.path.basename(image_path)), apply_averaging_blur(image))
    cv2.imwrite(os.path.join(folder_path, subfolders[2], os.path.basename(image_path)), add_salt_pepper_noise_based_on_bw(image, bw_image))

def process_images_multiproc_map(folder_path):
    subfolders = ['BlackWhite', 'Blurred', 'Noise']
    create_subfolders(folder_path, subfolders)
    tasks = [(os.path.join(folder_path, image_name), folder_path, subfolders) for image_name in os.listdir(folder_path) if image_name.lower().endswith('.jpg')]
    with multiprocessing.Pool(max(1, multiprocessing.cpu_count() - 2)) as pool:
      pool.map(process_image_map, tasks)

# sequential

def process_images_sequential(folder_path):
    subfolders = ['BlackWhite', 'Blurred', 'Noise']
    create_subfolders(folder_path, subfolders)
    for image_name in os.listdir(folder_path):
        if image_name.lower().endswith('.jpg'):
            image_path = os.path.join(folder_path, image_name)
            process_image(image_path, folder_path, subfolders)

# efficiency_calculations

import time
import pandas as pd

def measure_execution_time(func, path):
    start_time = time.time()
    func(path)
    end_time = time.time()
    return round(end_time - start_time, 2)

def calculate_metrics(sequential_time, parallel_time, n_processors):
    speedup = sequential_time / parallel_time
    efficiency = speedup / n_processors
    total_parallel_overhead = (n_processors * parallel_time) - sequential_time
    return speedup, total_parallel_overhead, efficiency

def create_execution_time_table(times_dict):
    df = pd.DataFrame(list(times_dict.items()), columns=['Method', 'Execution time (seconds)'])
    print("Execution times for all methods:")
    print(df)
    return df

def create_efficiency_metrics_table(sequential_time, times_dict, n_processors):
    metrics = []
    for method, parallel_time in times_dict.items():
        speedup = sequential_time / parallel_time
        efficiency = speedup / n_processors
        total_parallel_overhead = (n_processors * parallel_time) - sequential_time
        metrics.append([method, round(speedup, 2),  round(total_parallel_overhead, 2), round(efficiency, 2)])
    df = pd.DataFrame(metrics, columns=['Method', 'Speedup', 'Total parallel overhead', 'efficiency'])
    print("Efficiency metrics for parallel methods:")
    print(df)
    return df

# main

if __name__ == '__main__':
    folder_path = '/content/drive/MyDrive/Data/images'
    n_processors = multiprocessing.cpu_count()
    times_dict = {
        'Sequential': measure_execution_time(process_images_sequential, folder_path),
        'Starmap async': measure_execution_time(process_images_multiproc_starmap_async, folder_path),
        'Map': measure_execution_time(process_images_multiproc_map, folder_path),
        'Apply async': measure_execution_time(process_images_multiproc_apply_async, folder_path)
    }
    execution_time_table = create_execution_time_table(times_dict)
    sequential_time = times_dict.pop('Sequential')
    efficiency_metrics_table = create_efficiency_metrics_table(sequential_time, times_dict, n_processors)